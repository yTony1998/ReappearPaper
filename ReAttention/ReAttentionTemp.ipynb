{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5354adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b212/anaconda3/envs/pytorch_gpu/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff6b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, require_padding_mask=False, require_lens=False,\n",
    "                 batch_first=False):\n",
    "    \"\"\"List of sequences to padded sequences\n",
    "\n",
    "    Args:\n",
    "        sequences: List of sequences (N, D)\n",
    "        require_padding_mask:\n",
    "\n",
    "    Returns:\n",
    "        (padded_sequence, padding_mask), where\n",
    "           padded sequence has shape (N_max, B, D)\n",
    "           padding_mask will be none if require_padding_mask is False\n",
    "    \"\"\"\n",
    "    padded = nn.utils.rnn.pad_sequence(sequences, batch_first=batch_first)\n",
    "    padding_mask = None\n",
    "    padding_lens = None\n",
    "\n",
    "    if require_padding_mask:\n",
    "        B = len(sequences)\n",
    "        seq_lens = list(map(len, sequences))\n",
    "        padding_mask = torch.zeros((B, padded.shape[0]), dtype=torch.bool, device=padded.device)\n",
    "        for i, l in enumerate(seq_lens):\n",
    "            padding_mask[i, l:] = True\n",
    "\n",
    "    if require_lens:\n",
    "        padding_lens = [seq.shape[0] for seq in sequences]\n",
    "\n",
    "    return padded, padding_mask, padding_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8af6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2, 256])\n",
      "torch.Size([2, 100])\n",
      "torch.Size([120, 2, 256])\n",
      "torch.Size([2, 120])\n"
     ]
    }
   ],
   "source": [
    "a_len = torch.rand((80,256))\n",
    "b_len = torch.rand((100,256))\n",
    "\n",
    "c_len = torch.rand((80,256))\n",
    "d_len = torch.rand((120,256))\n",
    "\n",
    "sequence1 = [a_len,b_len]\n",
    "sequence2 = [c_len,d_len]\n",
    "padded_seq1, mask_seq1, _ = pad_sequence(sequences=sequence1, require_padding_mask=True)\n",
    "print(padded_seq1.shape)\n",
    "print(mask_seq1.shape)\n",
    "padded_seq2, mask_seq2, _ = pad_sequence(sequences=sequence2, require_padding_mask=True)\n",
    "print(padded_seq2.shape)\n",
    "print(mask_seq2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5a7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, num_heads=8, d_k=256, d_v=256):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        # head_dim = dim // num_heads\n",
    "        \n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = 1.0/(num_heads**0.5)\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.w_q = nn.Linear(dim, num_heads*d_k,bias=False)\n",
    "        self.w_k = nn.Linear(dim, num_heads*d_k,bias=False)\n",
    "        self.w_v = nn.Linear(dim, num_heads*d_v,bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "        self.reatten_matrix = nn.Conv2d(self.num_heads,self.num_heads, 1, 1)\n",
    "        self.var_norm = nn.BatchNorm2d(self.num_heads)\n",
    "        self.reatten_scale = self.scale\n",
    "\n",
    "        self.attn_drop = nn.Dropout(0.1)\n",
    "        self.proj = nn.Linear(num_heads*d_v, dim)\n",
    "        self.proj_drop = nn.Dropout(0.9)\n",
    "\n",
    "    \n",
    "    def Myattention(self, q, k, v, attn_mask):\n",
    "\n",
    "        B, num_head, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, num_head, Nt, E) x (B, num_head, E, Ns) -> (B, num_head, Nt, Ns)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = attn.softmax(dim=-1)       \n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        #  attn  (B, num_head, Nt, Ns)    \n",
    "        attn = self.var_norm(self.reatten_matrix(attn))*self.reatten_scale\n",
    "        \n",
    "#         atten = atten.view(B*num_heads, atten.shape[2], atten.shape[3])\n",
    "#         v = v.view(B*num_head, v.shape[2], v.shape[3])\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.matmul(attn, v)\n",
    "        return output, attn    \n",
    "    \n",
    "    def forward(self, q, k, v, key_padding_mask):\n",
    "\n",
    "        \n",
    "        Nq,B,Dq = q.shape\n",
    "        Nk,B,Dk = k.shape\n",
    "        Nv,B,Dv = v.shape\n",
    "        tgt_len, bsz, embed_dim = q.shape\n",
    "        src_len = k.shape[0]\n",
    "        bsz = key_padding_mask.shape[0]\n",
    "        attn_mask = key_padding_mask.view(bsz,1,1,src_len).expand(-1, self.num_heads, -1, -1)\n",
    "\n",
    "\n",
    "        new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n",
    "        new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n",
    "        attn_mask = new_attn_mask    \n",
    "\n",
    "        \n",
    "        newq = self.w_q(q).view(B,self.num_heads,Nq, self.d_k)\n",
    "        newk = self.w_q(k).view(B,self.num_heads,Nk, self.d_k)\n",
    "        newv = self.w_q(v).view(B,self.num_heads,Nv, self.d_v) \n",
    "\n",
    "        attn_output, attn_output_weights = self.Myattention(newq, newk, newv, attn_mask)\n",
    "\n",
    "        attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.d_k*self.num_heads)\n",
    "\n",
    "        attn_output = self.proj(attn_output)\n",
    "        return attn_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b5717eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2, 256])\n",
      "torch.Size([120, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "MyReAttention = ReAttention(dim=256)\n",
    "output = MyReAttention(q=padded_seq1,k=padded_seq1,v=padded_seq1,key_padding_mask=mask_seq1)\n",
    "print(output.shape)\n",
    "\n",
    "MyReAttention1 = ReAttention(dim=256)\n",
    "output1 = MyReAttention1(q=padded_seq2,k=padded_seq1,v=padded_seq1,key_padding_mask=mask_seq1)\n",
    "print(output1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b7510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
